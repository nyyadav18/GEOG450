# -*- coding: utf-8 -*-
"""Copy of Yelp_Web Crawler_reviews_Yifan.ipynb
BY Yifan Sun

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UIOHLag5jHSoa_yD75xTfC6n-ju4oW2D

**In the practice we will use requests+xpath+json to develop a simple but efficient Yelp web crawler**

Our ip address will be blocked if we run too fast or send too many request to the server of Yelp, resulting in returning error. If you encounter this kind of problem, please restart your server, enlarge the pause time and reduce the total numer (content & comments) you ask.

One advanced technique to bypass the anti-cralwer tech "IP block" is "IP pool", which means we collect a large number of IP addresses, and send requests using different IPs, so the workload for each IP is small, while one single IP address is less likely to be blocked, when one IP is blocked, we always have alternative IP to use. In this demo notebook, we wouldn't use any advanced tech.


### Key Knowledges:
- Workflow of a web crawler
  - send request-> recieve response-> extract needed information from the response->reformat and storage
- Web API
  - the form is like http://xxxx.xx.xx?param1=xxx&param2=xxxxx
  - in the example of Yelp we will use two different web API, one returns html, the other returns Json
- Request
  - send a request to a server and get the response
  - the content of the response can be json, html, image etc.
- Xpath
  - Strong tools to extraxt specific information from xml/html, and the libarary is lxml.
  - It first turn the html document/text to a element-tree, then we can use xpath grammar to extract element from the tree.
- Json
  - A key-value style data format, used widely in transmit web data
  - We use libary json in python to extract needed information from returned data of a web request

by Yifan
"""

"""
My explanation:
Yelp Data Fetcher and JSON Formatter
- use when specific visualization requires nested data
- organizes the data into a hierarchal structure
- works with cleaned CSV inputs
- focus is on data transformation
"""

#Read the following source code to learn basic skills of developing a web crawler

#pip install lxml requests json pandas
#For colab, lxml,requests,json,pandas are pre-installed
import requests
import lxml
import lxml.etree
import json
import pandas as pd
import time

#Construct a real-like request hearder, so the server is likely to consider it as a human being's request
useragent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36"
header = {
    'user-agent': useragent,
    'cookie': 'your cookie'
}


def getYelpSearchList(query="bar",location="Georgetown, WA 98108",max_pages=20):
  """Extract the detail page urls of Yelp search result (By Yifan)

  Retrieves the urls for detials in research result of Yelp, by giving the key word and location limitation.
  one example: https://www.yelp.com/search?find_desc=Restaurants&find_loc=Seattle, WA 98104&start=0

  Args:
    query: the key word used in searching, querying
    location: the area/location limitation for the query
    max_pages: the list is limited to 24 pages, resulting in maxinum 240 items; since frequent will be blocked, I don't think to try it in a large value
  Returns:
    A list, which contains the url for detalied pages

  """
  # construct the query url for Yelp
  # one example: https://www.yelp.com/search?find_desc=Restaurants&find_loc=Seattle, WA 98104&start=0
  # param- start: the start number of the result list, each pages contains 10 items, and the list is limited to 24 pages, resulting in maxinum 240 items
  starts=[i*5 for i in range(max_pages)]
  url_list=[]
  qstr="https://www.yelp.com/search?find_desc={}&find_loc={}&start=".format(query,location)
  for start in starts:
    url=qstr+str(start)
    url_list.append(url)

  #Extract the information in the first-level pages
  detail_urls=[]
  for url in url_list:
    print(url)
    time.sleep(0.5)
    response = requests.get(url, headers=header) # get the response of a request for a certain url, header is defined in the beginning
    html = lxml.etree.HTML(response.text) # turn the response's html text into a element-tree which then can be searched using xpath grammer
    if not html.xpath('//h3[contains(string(),"is unavailable")]'): # in case there is less than 240 items of our result, make the code robust
      itms=html.xpath('//h4/span[contains(string(),".")]/a')
      if itms is None:  # Frequent request causing IP blocking, then the return should be a error page, form which we can get nothing
          print('IP Block, your need to restart the server')
      for i in itms:
        detail_url="https://www.yelp.com/"+i.xpath('@href')[0]
        detail_urls.append(detail_url)
  return detail_urls

def getYelpDetails(detail_url="https://www.yelp.com/biz/the-sovereign-washington-3?osq=bar"):
  """Extract information from one specific Yelp detail page (By Yifan)

  Retrieves content_id,name,rate,review,tags,address,city,state,postcode of a bussiness place, by giving a detail page url.
  one example: https://www.yelp.com//biz/the-pink-door-seattle-4?osq=Restaurants

  Args:
    detail_url: url of Yelp detail page

  Returns:
    A list, which contains 9 items (content_id,name,rate,review,tags,address,city,state,postcode)
    the content_id is important - we need it to further request comments
  """

  #devle into detail_url to get more information
  response = requests.get(detail_url, headers=header)
  html = lxml.etree.HTML(response.text)

  try:
    name=html.xpath("//h1//text()")[0]
    address=html.xpath("//address/a/p//text()")[0]
    region=html.xpath("//address/p//text()")[-1]
    city=region.split(',')[0]
    state=region.split(',')[1].split()[0]
    postcode=region.split(',')[1].split()[1]
    rate=float(html.xpath("//h1/../../div[2]/div[1]//div[@role='img']/@aria-label")[0].split()[0])
    review=int(html.xpath("//h1/../../div[2]/div[2]//span/text()")[0].split()[0])
    content_id=html.xpath("//meta[@name='yelp-biz-id']/@content")[0]
    tags=';'.join(html.xpath("//h1/../../span[3]//a//text()"))
  except Exception as e:
    print('IP blocked:'+detail_url,e)
    content_id,name,rate,review,tags,address,city,state,postcode=None,None,None,None,None,None,None,None,None
  return [content_id,name,rate,review,tags,address,city,state,postcode]



def getYelpComments(content_id="VOPdG8llLPaga9iJxXcMuQ",max_comments=10,pause=0.7):
  """Extract comments of one specific content in Yelp (By Yifan)

  Retrieves content_id,name,rate,review,tags,address,city,state,postcode of a bussiness place, by giving a detail page url.
  one example: https://www.yelp.com//biz/the-pink-door-seattle-4?osq=Restaurants

  Args:
    content_id: the Yelp id of the content, it's a string and can be find when examine the source code of a detail page
    max_comments: how many comments do you want? It can be a larger number, since when it's larger than total comments num, we will use total comments num
    pause: speed contral, single pause time (s) for avoiding IP blocking

  Returns:
    A list, which contains 9 items (content_id,name,rate,review,tags,address,city,state,postcode)
    the content_id is important - we need it to further request comments
  """

  def extractComments(content_id='VOPdG8llLPaga9iJxXcMuQ',page=0):
    #example of a detail page: https://www.yelp.com//biz/the-pink-door-seattle-4?osq=Restaurants
    #By analyzing the traffic flow of the detail page of Yelp, we find the data package of the dynamic page
    #the target data package API: https://www.yelp.com/biz/VOPdG8llLPaga9iJxXcMuQ/review_feed?rl=en&q=&sort_by=relevance_desc&start=0
    url="https://www.yelp.com/biz/{}/review_feed?rl=en&q=&sort_by=relevance_desc&start={}".format(content_id,page*10)
    response = requests.get(url, headers=header)
    data = json.loads(response.text) #term json string to dictionary

    total_num=data['pagination']['totalResults']
    rst=[]
    page_num=len(data['reviews'])
    for i in range(page_num):
      cmt=data['reviews'][i]
      comment_id=cmt['id']
      user_id=cmt['userId']
      user_name=cmt['user']['markupDisplayName']
      location=cmt['user']['displayLocation'].split(',')
      if len(location)>1:
        display_city=location[0]
        display_state=location[1]
      elif len(location)>0:
        display_city=location[0]
        display_state=None
      else:
        display_city,display_state=None,None
      rating=cmt['rating']
      date=cmt['localizedDate']
      if 'feedback' in cmt and 'counts' in cmt['feedback']:
        feedback_useful=cmt['feedback']['counts']['useful']
        feedback_funny=cmt['feedback']['counts']['funny']
        feedback_cool=cmt['feedback']['counts']['cool']
      else:
        feedback_useful,feedback_funny,feedback_cool=None,None,None
      if 'comment' in cmt and 'text' in cmt['comment']:
        comment=cmt['comment']['text']
      else:
        comment=None
      rst.append([content_id,comment_id,user_id,user_name,display_city,display_state,rating,date,feedback_useful,feedback_funny,feedback_cool,comment])
    return total_num,rst

  total_num,trst=extractComments(content_id,0)
  if total_num<max_comments:
    max_comments=total_num
  if max_comments%10==0:
    maxPages=int(max_comments/10)
  else:
    maxPages=int(max_comments/10)+1
  #start of the outer function
  if maxPages>1:
    print('********start to extract comments of {}********'.format(content_id))
  rst=[]
  rst=rst+trst
  for page in range(maxPages-1):
    page=page+1
    print("coments: page {}/{}".format(page+1,maxPages))
    time.sleep(pause)
    try:
      _,trst=extractComments(content_id,page)
    except Exception as e:
      print('IP Blocked')
      time.pause(pause*5)
      try:
        _,trst=extractComments(content_id,page)
      except Exception as e:
        trst=[None]*12
    rst=rst+trst
  rst=rst[0:max_comments]
  if maxPages>1:
    print('********finish extractting comments of {}********'.format(content_id))
  return rst



def getYelpInfo_noComments(query="bar",location="Georgetown, WA 98108",max_pages=3,pause=0.7):
  """Download formatted Yelp search results without comments(By Yifan)

  Retrieves basic information of a Yelp Search. Your can use the function directly to get max 240 records of one single Yelp search

  Args:
    query: the key word used in searching, querying
    location: the area/location limitation for the query
    max_pages: the list is limited to 24 pages, resulting in maxinum 240 items; since frequent will be blocked, I don't think to try it in a large value
    pause: speed contral, single pause time (s) for avoiding IP blocking
  Returns:
    A pandas dataframe, which contains 9 columns ('content_id','name','rate','review','tags','address','city','state','postcode')
  """
  detail_urls=getYelpSearchList(query,location,max_pages)
  print('*****Extracting {} at {}****'.format(query,location))
  num=len(detail_urls)
  rst=[]
  for i,detail_url in zip(range(num),detail_urls):
    if (i+1)%5==0:
      print('Retrieve Info: {}/{}'.format(i+1,num))  #It's a nice habit to print status of your web crawler
      time.sleep(pause*5)
    trst=getYelpDetails(detail_url)
    if trst[0]==None:
      print('IP Block: '+detail_url)
      time.sleep(pause)
    rst.append(trst)
  print('***finish****')
  df=pd.DataFrame(data=rst,columns=['content_id','name','rate','review','tags','address','city','state','postcode'])  # re-format your results to a dataframe, if your like you can download the results or upload them to your remote databse
  df=df.loc[df['content_id'].notnull()] #drop none records which may be caused by IP blocking
  return df


def getYelpInfo_includeComments(query="bar",location="georgetown, WA 98108",max_pages=3,max_comments=3,pause=0.7):
  """Download formatted Yelp search results with comments(By Yifan)

  Retrieves basic information of a Yelp Search. Your can use the function directly to get max 240 place items and infinite comments with them (as long as there is a comment)
  However, without advanced tech called IP pool, I don't suggest you dowload a large amount of comments.....

  Args:
    query: the key word used in searching, querying
    location: the area/location limitation for the query
    max_pages: the list is limited to 24 pages, resulting in maxinum 240 items; since frequent will be blocked, I don't think to try it in a large value
    max_comments: how many comments do you want for one single content? It can be a larger number, since when it's larger than total comments num, we will use total comments num
    pause: speed contral, single pause time (s) for avoiding IP blocking
  Returns:
    A pandas dataframe, which contains 20 columns ('content_id','name','rate','review','tags','address','city','state','postcode','comment_id','user_id','user_name','display_city','display_state','rating','date','feedback_useful','feedback_funny','feedback_cool','comment')
  """

  tdf=getYelpInfo_noComments(query,location,max_pages,pause)
  rst=[]
  for content_id in tdf['content_id']:
    trst=getYelpComments(content_id,max_comments=3,pause=0.7)
    rst=rst+trst
  cdf=pd.DataFrame(data=rst,columns=['content_id','comment_id','user_id','user_name','display_city','display_state','rating','date','feedback_useful','feedback_funny','feedback_cool','comment'])
  cdf=cdf.loc[cdf['content_id'].notnull()]
  df=pd.merge(tdf,cdf,on='content_id')
  return df

#Excute Demo: Extract the detail page urls of Yelp search result
getYelpSearchList(query="bar",location="Georgetown, WA 98108",max_pages=2)

#Excute Demo: Extract information from one specific Yelp detail page
getYelpDetails(detail_url="https://www.yelp.com/biz/the-sovereign-washington-3?osq=bar")

#Excute Demo: Extract information from one specific Yelp detail page
getYelpDetails(detail_url="https://www.yelp.com/biz/clydes-of-georgetown-washington-3?osq=bar")

#Excute Demo: Extract comments of one specific content in Yelp
rst=getYelpComments(content_id="c1xpqRI-4t1rKGqFxXg2aQ",max_comments=30,pause=0.5)
sdf=pd.DataFrame(data=rst,columns=['content_id','comment_id','user_id','user_name','display_city','display_state','rating','date','feedback_useful','feedback_funny','feedback_cool','comment'])
sdf

#Excute Demo: Download formatted Yelp search results without comments
df1=getYelpInfo_noComments(query="bar",location="georgetown, WA 98108",max_pages=2,pause=0.8)
df1

#Excute Demo: Download formatted Yelp search results with comments
rdf=getYelpInfo_includeComments(query="bar",location="georgetown, WA 98108",max_pages=1,max_comments=3,pause=0)
rdf
